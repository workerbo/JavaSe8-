---
typora-copy-images-to: assert
typora-root-url: assert
---

[TOC]

# 重要命令

**查看kubelet日志：**journalctl -u kubelet
**查看名称空间：**kubectl get ns
**查看pod（必须在指定名称空间下查看）：**kubectl get pod
**查看所有名称空间的pod：**kubectl get pods --all-namespaces
**查看所有名称空间的pod：**kubectl get pods --all-namespaces -o wide
**查看集群内所有节点：**kubectl get nodes
**master监控Node各节点：**watch kubectl get pod -n kube-system -o wide



# 一、安装kubernetes集群

## 1.安装基础环境

### 安装虚拟机

![1641648625504](/1641648625504.png)



![1641648689796](/1641648689796.png)

```
1.拷贝k8s文件夹到一个无空格、中文的目录下，修改Vagrantfile的node.vm.box = "centos7"
	因为之前vagrant box add centos7 CentOS-7-x86_64-Vagrant-2004_01.VirtualBox.box给本地设置了一个centos7，否则使用centos/7会去仓库下载会很慢
	
2.修改每个node，使其可以远程连接
	vagrant ssh k8s-node1
	su root   密码：vagrant
	vi /etc/ssh/sshd_config	修改PasswordAuthentication yes
	service sshd restart【reboot】
	xshell连接，192.168.56.100 -- root + vagrant
	
3.NAT网络
	存在的问题：
		vagrant创建的虚拟机默认是网络地址转换，所有虚拟机公用一个IP，由宿主主机本地端口与虚拟机端口映射实现绑定。例如宿主主机的13306端口绑定虚拟机的3306端口（会同时映射3台）
【使用ip route show查看每个虚拟机使用的都是eth0网卡，然后输入ip addr，每个虚拟机的eth0网卡都是10.0.2.215这个ip，原因是都使用的是网络地址转换NAT（三台虚拟机共用一个ip）】
	网卡1：k8s集群使用
	网卡2：仅主机模式，保证宿主机与k8s在同一局域网内 192.168.56.xx
	设置：
		1）全部关闭电源
		2）全局设定=》网络=》创建NAT 网络
		3）选中node1机器（每个机器都要设置）=》网络=》网卡1=》选择NAT网络（选中刚刚创建的NAT 网络）=》高级=》刷新mac地址【每台虚拟机都会生成一个IP】
	
4.记录每台虚拟机的ip
ip addr【查看网卡1就是eth0的ip】
192.168.56.100=》10.0.2.4
192.168.56.101=》10.0.2.15
192.168.56.102=》10.0.2.5

5.设置linux环境
关闭防火墙:
	systemctl stop firewalld
	systemctl disable firewalld
关闭selinux:【默认安全策略 cat /etc/selinux/config】
	sed -i 's/enforcing/disabled/' /etc/selinux/config
	setenforce 0
关闭swap: 关闭内存交换【cat /etc/fstab】
	swapoff -a	只是临时关闭
	sed -ri 's/.*swap.*/#&/' /etc/fstab   选择这个
	
6.修改主机名
	1）查看主机名，如果主机名是localhost需要修改
		hostname
	2）修改
		hostnamectl set-hostname k8s-node1

7.修改每个虚拟机的hosts【集群间域名访问】
vi /etc/hosts
10.0.2.4 k8s-node1
10.0.2.15 k8s-node2
10.0.2.5 k8s-node3

8.将桥接的IPv4流量传递到iptables的链:
cat > /etc/sysctl.d/k8s.conf << EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

sysctl --system


疑难问题:
遇见提示是只读的文件系统，运行如下命令
mount -o remount rw /

date查看时间(可选)
yum install -y ntpdate
ntpdate time.windows.com同步最新时间


备份：
给三台主机做备份
```

### 安装docker

```json
1、卸载旧版本【uninstall old versions】
	sudo yum remove docker \
                  docker-client \
                  docker-client-latest \
                  docker-common \
                  docker-latest \
                  docker-latest-logrotate \
                  docker-logrotate \
                  docker-engine
                  
2、安装前置依赖
sudo yum install -y yum-utils device-mapper-persistent-data lvm2

3、设置镜像地址
	方案一：
	sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
	方案二：（推荐，阿里镜像）
	sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo【这个镜像和后面提到的镜像加速不同，这里指的是下载docker本身的镜像地址】
	
4、安装docker以及docker-cli
sudo yum install -y docker-ce docker-ce-cli containerd.io

5、虚拟机开机启动：
	sudo systemctl enable docker
启动docker
	sudo systemctl start docker


6、测试
docker images -a

7、docker配置镜像下载加速
	1）默认从hub.docker下载软件镜像【很慢】
	2）修改成aliyun的镜像加速器
	3）执行【获取第一步配置的镜像加速器网址】
sudo tee /etc/docker/daemon.json <<-'EOF'
{
  "registry-mirrors": ["https://7zi5cb9i.mirror.aliyuncs.com"]
}
EOF

sudo systemctl daemon-reload
sudo systemctl restart docker
	
8.拉取
docker pull quay.io/openebs/provisioner-localpv:1.5.0
```

### 安装kubeadmin、kubelet、kubectl

![1644334589330](/1644334589330.png)

```json
1.每个节点都要安装Docker（master和Node都要）
2.每个节点安装kubeadmin（可以快速构建集群环境，设置节点为Master，将节点加入到Master的Node）
3.每个节点安装kubelet
4.每个节点安装kubectl（避免使用命令行执行API）
```

```
添加阿里云yum源：

cat > /etc/yum.repos.d/kubernetes.repo << EOF
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
```

```
查看是否安装相关源：yum list|grep kube
1、安装
yum install -y kubelet-1.17.3 kubeadm-1.17.3 kubectl-1.17.3

2、开机启动kubelet，要把每一个Node注册到集群中
sudo systemctl enable kubelet
sudo systemctl start kubelet【到这一步暂时还启动不起来：systemctl status kubelet，后面还有要配置的】
```

### 重置集群

```sh
当执行了以下两条命令时，需要执行底部重置集群的命令，正常安装可跳过重置集群的操作：
swapoff -a
kubeadm reset
	文档：https://blog.csdn.net/woay2008/article/details/93250137


Master执行：
systemctl daemon-reload
systemctl restart kubelet
iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X  
rm -rf $HOME/.kube

Master+Node执行：
     rm -rf /var/lib/etcd
	 rm -rf /etc/cni/net.d
	 rm -rf /var/lib/kubelet
	 rm -rf /etc/kubernetes
```

## 2.部署

### 部署Master

```
1.挑选一台主机为master【当前选择10.0.2.4作为主机】
将k8s文件夹上传到master主机的root路径下（）

2.执行可执行文件拉取镜像（master_images.sh：rw- 没有执行权限【root没有执行权限】）
	1）chmod 700 master_images.sh  （rwx）
	2）./master_images.sh【https://blog.csdn.net/youzhouliu/article/details/79051516】
	3）docker images 查看镜像下载情况


3、挑选一台主机为master【当前选择10.0.2.4作为主机】
sudo kubeadm init \
--apiserver-advertise-address=10.0.2.4 \
--image-repository registry.cn-hangzhou.aliyuncs.com/google_containers \
--kubernetes-version v1.17.3 \
--service-cidr=10.96.0.0/16 \
--pod-network-cidr=10.244.0.0/16

打印Your Kubernetes control-plane has initialized successfully!代表初始化成功

由于默认拉取镜像地址k8s.gcr.io国内无法访问，这里指定阿里云镜像仓库地址。可以手动按照我们的执行master_images.sh先拉取镜像，地址变为registry.aliyuncs.com/google_containers也可以。

科普:无类别域间路由(Classless Inter-Domain Routing、CIDR)是一个用于给用户分配IP地址以及在互联网上有效地路由IP数据包的对IP地址进行归类的方法。【1个Node有多个service，1个service有多个pod，1个pod有多个容器】
拉取可能失败，需要下载镜像。

运行完成提前复制:加入集群的令牌

pod-network-cidr：docker最小的单位是容器，pod是k8s中最小的单位（多个容器组成）
service-cidr：service是一组pod组成对外提供服务，负载均衡

4.记录上一步token
kubeadm join 10.0.2.4:6443 --token rbbfs6.or9h6i8mgdfm1czl \
    --discovery-token-ca-cert-hash sha256:9000449fd492240244794f13aab5dbd99e939dc80650b378ebee27e9c0a7f851 
【如果token过期，重新获取一个没有过期时间的token（默认2h过期）：
kubeadm token create --print-join-command --ttl=0】    

5、 生成配置文件
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

### 安装pod网络插件（CNI）

![1641658946879](/1641658946879.png)

```
部署pod网络【只有部署了网络，Node才可以加入进来】
参照doc：https://kubernetes.io/docs/concepts/cluster-administration/addons/
使用Flannel： 是一个可以用于 Kubernetes 的 overlay 网络提供者。


执行
kubectl apply -f \
https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
这个地址被墙，使用我们k8s文件夹中的yml即可，同时flannel.yml中指定的images访问不到可以去docker hub找一个

1、执行使用本地的flannel.yml【先拉取镜像】
	1）如果kube-flannel.yml中指定的images访问不到去docker.hub中查找并修改yml中的配置
		# 1、访问hub.docker.com，搜索flannel，点击from quay.io/coreos/flannel
		# 2、拷贝v0.11.0-amd64的来源jmgao1983/flannel:v0.11.0-amd64
		# 3、flannel.yml中image指定的地方，修改
	2）也可以使用quay-mirror.qiniu.com和registry.aliyuncs.com
	例如quay.io=》quay-mirror.qiniu.com  || gcr.io改为registry.aliyuncs.com

下拉镜像：quay.io/coreos/flannel:v0.10.0-s390x
如果拉取较慢，可以改为：quay-mirror.qiniu.com/coreos/flannel:v0.10.0-s390x

下拉镜像：gcr.io/google_containers/kube-proxy
可以改为： registry.aliyuncs.com/google_containers/kube-proxy

2、执行kubectl apply -f kube-flannel.yml
使用本地的yml规则安装pod组件【kubectl delete -f kube-flannel.yml：删除yml中配置的所有组件重新安装】

3、查看名称空间
kubectl get ns

4、获取所有名称空间的pod
kubectl get pods --all-namespaces -o wide
如果全部都是running就成功了，如果有不是running状态的等待一段时间，下载好镜像后就可以了

docker images|grep flannel可以查到以下镜像：
jmgao1983/flannel                                                             v0.11.0-amd64       ff281650a721        19 months ago       52.6MB

```

### 将从节点join到k8s集群

```
1、当master是ready状态时，在k8s-node2、k8s-node3执行以下命令将其添加到master
kubeadm join 10.0.2.4:6443 --token rbbfs6.or9h6i8mgdfm1czl \
    --discovery-token-ca-cert-hash sha256:9000449fd492240244794f13aab5dbd99e939dc80650b378ebee27e9c0a7f851 

异常解决：
2.master执行watch kubectl get pod -n kube-system -o wide发现问题
文档：https://www.jianshu.com/p/bcc05427990d（CrashLoopBackOff）
	1）找到出问题的主机执行kubectl get pod获取pod的状态
	2）查看pod的详细信息：kubectl describe pod elkhost-944bcbcd4-8n9nj
	3）查看此pod日志：kubectl logs elkhost-944bcbcd4-8n9nj
	
3、master监控Node各节点
watch kubectl get pod -n kube-system -o wide
查看到node2和node3镜像下载失败
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.17.3
docker pull kubesphere/kube-proxy:v1.17.3
docker pull jmgao1983/flannel:v0.11.0-amd64
```

## 3.入门操作体验

### 部署tomcat

```
1.根据镜像创建一个部署：根据tomcat镜像（带java环境的镜像）创建一个部署（名字叫tomcat6）
kubectl create deployment tomcat6 --image=tomcat:6.0.53-jre8【1个部署对应1个pod对应1个容器】
	解析：k8s会通过主节点调用其他节点下载镜像启动容器
	

Kubectl get pods -o wide可以获取到tomcat信息【Kubectl get all】
# 容灾恢复1：在k8s-node3执行docker stop tomcat6停掉该容器，kubernetes会给node3重启一个容器
# 容灾恢复2：如果k8s-node3宕机，master会找到集群中另一个节点启动pod
```



### 暴露nginx访问



### 动态扩容测试